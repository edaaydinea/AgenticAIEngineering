{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb84e170",
   "metadata": {},
   "source": [
    "# Day 1 - Autonomous AI Agent Demo: Using N8n to Control Smart Home Devices\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "This course introduction, led by instructor Ed Donner, uniquely bypasses traditional opening remarks to immediately demonstrate an autonomous AI agent using n8n, a low-code/no-code workflow automation platform with built-in generative AI. The demo showcases an AI agent, powered by an OpenAI LLM (with Olama as an alternative), using a \"Philips Hue\" tool to control smart lights, including making a simple decision like choosing a light color. This practical example sets the stage for the main focus of the six-week, eight-project course: teaching students to move beyond using such tools and instead learn how to code and build sophisticated AI agents and orchestration frameworks themselves.\n",
    "\n",
    "### **Highlights**\n",
    "\n",
    "- **Unconventional \"Action-First\" Introduction**: The course immediately engages students with a practical demonstration of an AI agent using the n8n platform, deferring traditional introductions and course goal discussions to highlight tangible AI capabilities from the outset.\n",
    "- **n8n as a Low-Code/No-Code AI Workflow Platform**: n8n is introduced as a tool that simplifies the creation of automated workflows by connecting different applications and services, with a distinctive feature being its integration of generative AI. It can be used via n8n.cloud or run locally.\n",
    "- **Core Components of the Demonstrated AI Agent**: The agent built in the n8n demo illustrates a typical agent structure:\n",
    "    - **Trigger**: A user chat message initiates the workflow.\n",
    "    - **Agent Node**: An \"AI Agent\" component that orchestrates tasks.\n",
    "    - **Chat Model (LLM)**: An underlying Large Language Model (OpenAI Chat Model used in demo; Olama presented as a free, local alternative) provides reasoning capabilities. This requires API key setup for services like OpenAI.\n",
    "    - **Tool Integration**: The agent is equipped with \"tools\" to interact with external systems; the demo used a \"Philips Hue\" tool to control smart lights.\n",
    "- **Significance of \"Tools\" for AI Agents**: Tools are emphasized as fundamental building blocks for AI agents, enabling them to perform actions beyond text generation, such as calculations, interacting with calendars (Google Calendar), social media (Facebook), news feeds (Hacker News), or IoT devices (Philips Hue lights).\n",
    "- **Demonstrating Basic Agent Autonomy**: The demo illustrated a simple form of agent autonomy where, given a choice between two light colors (deep red or deep blue), the agent independently selected one and executed the change, moving beyond mere instruction following.\n",
    "- **Transition from User to Developer Focus**: While n8n is used for the introductory demo to give a \"hands-on sense\" of agentic AI, the instructor explicitly states this is the last time such a low-code tool will be used \"as a user.\" The core of the course will involve \"rolling up our sleeves and coding agents\" and building frameworks for agent orchestration.\n",
    "- **Course Structure and Objective**: The six-week course will involve eight distinct projects designed to teach students how to build AI agents and systems where multiple agents can interact and be orchestrated to solve complex problems.\n",
    "- **Real-World Interaction Capability**: The agent controlling physical smart lights provides a clear and engaging example of how AI agents can extend their influence beyond digital environments to interact with and manipulate the physical world.\n",
    "- **Concept of Orchestration**: The n8n platform itself serves as an example of workflow orchestration. The course aims to teach students how to build more complex orchestration frameworks for AI agents programmatically.\n",
    "\n",
    "### **Conceptual Understanding**\n",
    "\n",
    "- **Autonomous AI Agents (Illustrated by the n8n Demo)**\n",
    "    1. **Why is this concept important?** AI agents represent a paradigm shift from passive AI models to active systems that can perceive their (limited) environment, make decisions, and take actions to achieve specified goals with a degree of autonomy. Understanding their fundamental components—an LLM for reasoning, a set of tools for action, and a mechanism for decision-making—is crucial for developing next-generation AI applications that can actively assist users or automate complex tasks.\n",
    "    2. **How does it connect to real-world tasks, problems, or applications?** The simple demo of an agent choosing a light color and changing it based on a user's open-ended request (\"pick a color... either deep red or deep blue\") illustrates the basic loop: perceive (user's instruction and options) -> decide (select \"red\") -> act (command the Philips Hue tool). This pattern scales to more complex applications:\n",
    "        - **Automated research:** An agent could be tasked to find information on a topic, decide which sources are most relevant using search tools, and compile a summary.\n",
    "        - **Smart assistants:** Managing schedules by interacting with calendar tools, making decisions about meeting conflicts based on priorities.\n",
    "        - **E-commerce bots:** Guiding users through a purchase, using tools to check inventory, and making decisions on product recommendations.\n",
    "        - **Software development:** Agents that can write code, use debugging tools, and make decisions on how to fix errors.\n",
    "    3. **Which related techniques or areas should be studied alongside this concept?**\n",
    "        - **Large Language Model (LLM) Tool Use / Function Calling:** Understanding how LLMs are specifically prompted or fine-tuned to reliably select and use external tools or APIs based on a user's request or a given task.\n",
    "        - **Planning and Reasoning in AI:** Exploring how agents can decompose complex goals into sequential or parallel sub-tasks and make choices among different courses of action.\n",
    "        - **Agent Architectures:** Studying different designs for agents, such as ReAct (Reason + Act) or more complex belief-desire-intention (BDI) models.\n",
    "        - **Multi-Agent Systems (MAS):** Learning how multiple specialized agents can collaborate, negotiate, or be orchestrated to solve problems that are beyond the capability of a single agent (as hinted by the course's future direction).\n",
    "        - **Agent Development Frameworks:** Becoming proficient in libraries and frameworks specifically designed for building AI agents (e.g., LangChain, AutoGen, Microsoft Semantic Kernel), which the course aims to cover through coding.\n",
    "\n",
    "### **Reflective Questions**\n",
    "\n",
    "1. **Application:** The n8n demo featured an AI agent using a Philips Hue light as a \"tool.\" If you were to design an AI agent to help a data scientist automate part of their model monitoring workflow, what existing software or service could act as a \"tool\" for this agent?\n",
    "    - *Answer:* An AI agent for model monitoring could use a \"tool\" like a connection to a Prometheus/Grafana API (to fetch performance metrics), a Slack or email client API (to send alerts), or even a GitHub API (to log issues or trigger retraining workflows if performance degrades significantly).\n",
    "2. **Teaching:** How would you explain the difference between the AI agent's first action (turning lights \"bright white\" on command) and its second action (choosing \"red\" when given \"red or blue\") to someone new to AI, to illustrate the concept of \"autonomy\"?\n",
    "    - *Answer:* The first action was like directly following a very specific recipe: \"turn lights bright white.\" The second action was more like being given two ingredients, red or blue, and a general instruction to \"pick one and make something\"; the agent had to make a choice itself (it picked red) instead of just executing a precise, predetermined command, showing a small step towards autonomy.\n",
    "3. **Extension:** The course will move from using n8n to \"coding agents\" and frameworks for their orchestration. What kind of complex, multi-step task in a typical business environment could benefit from multiple orchestrated AI agents, each potentially specializing in a different function?\n",
    "    - *Answer:* Processing a complex customer order in an e-commerce business could benefit from orchestrated agents: one agent could interact with the customer for order details, another could check inventory levels using a database tool, a third could process payment via a payment gateway tool, a fourth could arrange shipping by interacting with a logistics API, and a \"supervisor\" agent could oversee the entire process, ensuring each step is completed successfully and handling exceptions.\n",
    "\n",
    "# Day 1 - AI Agent Frameworks Explained: OpenAI SDK, Crew AI, LandGraph & AutoGen\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "This intensive six-week course, led by AI expert Ed Donner, aims to provide a comprehensive education in engineering AI agents by combining theoretical understanding with practical, hands-on projects. Students will explore a range of influential frameworks, including the OpenAI Agents SDK, CrewAI, LangGraph, Microsoft Autogen, and Anthropic's Model Context Protocol (MCP), progressively building their skills to tackle complex applications such as an agentic software engineering team, an interactive browser sidekick, an agent that creates other agents, and a capstone financial markets trading simulation.\n",
    "\n",
    "### **Highlights**\n",
    "\n",
    "- **Comprehensive 6-Week Curriculum**: The course is structured into six progressive weekly modules, each containing five days of lectures, designed to systematically develop expertise in AI agent engineering from foundational concepts to advanced applications.\n",
    "- **Balanced Learning Approach**: Learning is fostered through a three-pronged strategy: exploring the **theory** behind agentic architectures, gaining proficiency with various industry **frameworks** (OpenAI Agents SDK, CrewAI, LangGraph, Autogen, MCP), and building a portfolio of hands-on **projects** that have both demonstrative and commercial relevance.\n",
    "- **Gradual Introduction to Key Frameworks**: The course strategically introduces different AI agent frameworks, allowing students to grasp diverse paradigms:\n",
    "    - **Week 1**: Begins with native LLM interactions to build foundational understanding without framework abstractions.\n",
    "    - **Week 2**: Introduces the **OpenAI Agents SDK**, noted for its simplicity and flexibility.\n",
    "    - **Week 3**: Focuses on **CrewAI**, a popular low-code, configuration-driven framework.\n",
    "    - **Week 4**: Explores **LangGraph**, a powerful, full-code framework for complex agentic workflows.\n",
    "    - **Week 5**: Delves into **Microsoft Autogen**, emphasizing its capabilities for multi-agent collaboration.\n",
    "    - **Week 6**: Culminates with **Anthropic's Model Context Protocol (MCP)**, an open-source standard for inter-model communication, and a capstone project.\n",
    "- **Challenging and Innovative Projects**: The course features eight projects with increasing complexity, including several ambitious end-of-course applications:\n",
    "    - **Project 5**: An agentic platform simulating a software engineering team (front-end, back-end, lead, tester) that collaborates to write code.\n",
    "    - **Project 6**: A \"Sidekick\" agent that can interact with a web browser alongside the user, similar to emerging agentic AI startups.\n",
    "    - **Project 7**: A \"Creator\" agent, an agentic framework capable of generating new AI agents.\n",
    "    - **Project 8 (Capstone)**: A financial markets trading simulation where multiple agents make investment decisions based on real-time financial news, stock prices, and company reports (with some internal code written by Project 5 agents).\n",
    "- **Focus on Commercial Relevance and Skill Application**: All projects are designed with two goals: to be highly educational, enhancing students' technical capabilities, and to be commercially insightful, providing ideas for applying these skills in real-world business scenarios (B2B or B2C).\n",
    "- **Model Context Protocol (MCP) as a Capstone Concept**: The introduction of Anthropic's MCP in the final week signifies a focus on cutting-edge, open-source standards for enabling collaboration and capability sharing among different AI models.\n",
    "- **Experienced and Accomplished Instructor**: The course is delivered by Ed Donner, a professional with significant industry experience, including co-founding two AI startups and holding a Managing Director role at J.P. Morgan, bringing practical insights to the curriculum.\n",
    "\n",
    "### **Conceptual Understanding**\n",
    "\n",
    "- **Learning Diverse AI Agent Frameworks (OpenAI SDK, CrewAI, LangGraph, Autogen, MCP)**\n",
    "    1. **Why is this concept important?** The AI agent landscape is dynamic and features a variety of tools catering to different needs and complexities. Proficiency across multiple frameworks—such as the OpenAI Agents SDK (for focused, often OpenAI-centric agent development), CrewAI (for rapid prototyping and simpler, role-based agent configurations), LangGraph (for intricate control over agent state and execution flow in complex applications), Autogen (for facilitating conversations and collaborations between multiple agents), and MCP (for standardized inter-model communication)—provides a developer with a versatile skill set. This adaptability is crucial for selecting the optimal approach for a given project and for staying current in a fast-evolving field.\n",
    "    2. **How does it connect to real-world tasks, problems, or applications?** The choice of framework directly impacts development speed, scalability, and the type of agentic behavior that can be easily implemented:\n",
    "        - For quickly building a team of agents with predefined roles to automate a content creation pipeline, **CrewAI** might be suitable.\n",
    "        - For developing a complex research agent that requires careful management of long-running tasks, memory, and conditional logic, **LangGraph** offers the necessary power and control.\n",
    "        - When creating an ecosystem where multiple specialized AI services or models need to seamlessly exchange information and delegate tasks, **MCP** aims to provide a common language.\n",
    "        - If the goal is to simulate or build systems where agents with different personas and capabilities must negotiate or work together, **Autogen** provides robust tools.\n",
    "        This breadth of knowledge enables developers to tackle a wider spectrum of real-world agentic AI challenges effectively.\n",
    "    3. **Which related techniques or areas should be studied alongside this concept?**\n",
    "        - **Software Architecture for Distributed Systems:** Many multi-agent systems share characteristics with distributed computing.\n",
    "        - **API Design and Consumption:** Agents frequently rely on external tools and services accessed via APIs.\n",
    "        - **State Management Strategies:** Essential for agents that maintain context over multiple interactions or steps.\n",
    "        - **Testing and Debugging for Autonomous Systems:** Developing methodologies to validate and troubleshoot the behavior of complex, non-deterministic agents.\n",
    "        - **Comparative Framework Analysis:** Building the skill to critically evaluate new agent frameworks as they emerge, assessing their strengths, weaknesses, and suitability for different tasks.\n",
    "\n",
    "### **Reflective Questions**\n",
    "\n",
    "1. **Application:** Given the course's progression from native LLM interactions to sophisticated frameworks like LangGraph and Autogen, for a project that requires building a highly customized AI research assistant that can autonomously explore niche academic databases via their APIs and synthesize novel hypotheses, which framework learned later in the course would likely be most appropriate, and why?\n",
    "    - *Answer:* LangGraph would likely be most appropriate for such a project because it offers fine-grained control over the agent's execution flow and state management, which is crucial for complex, multi-step research tasks involving API interactions, data synthesis, and hypothesis generation. Autogen could also be relevant if the assistant is envisioned as a collaboration between multiple specialized \"researcher\" agents.\n",
    "2. **Teaching:** How would you explain the value of building Project 5 (the agentic software engineering team) to a business stakeholder who is curious about the practical ROI of AI agents beyond simple automation?\n",
    "    - *Answer:* Project 5 demonstrates that AI agents can go beyond automating simple, repetitive tasks; they can simulate and eventually augment collaborative, knowledge-intensive work, like software development. The ROI here isn't just about individual task automation, but about potentially accelerating complex project timelines, scaling development efforts, or even having AI assist in generating and testing code, thereby reducing development costs and speeding up innovation cycles for the business.\n",
    "3. **Extension:** The capstone project is a financial markets trading simulation. What is one significant real-world challenge or limitation (beyond being purely illustrative and not for real investment) that such an agentic system would face if deployed, even if technically proficient?\n",
    "    - *Answer:* A significant real-world challenge would be the \"black swan\" event or unforeseen market shock that falls outside the patterns learned from historical data or commonly reported news. Agentic systems, even those processing vast amounts of information, might struggle to react appropriately to truly novel situations for which they have no precedent, potentially leading to significant and unexpected losses or erratic behavior.\n",
    "\n",
    "# Day 1 - Agent Engineering Setup: Understanding Cursor IDE, UV & API Options\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "This introductory segment by instructor Ed Donner outlines the AI agent engineering course's inclusive design, catering to learners from coding novices to experienced agent engineers, with specific guidance for each group. Key practical aspects include setting up the development environment using the modern Cursor IDE and the fast, Rust-based UV package manager (replacing Anaconda), alongside a transparent discussion on managing API costs for frontier models with free local alternatives like Olama. The instructor emphasizes the importance of comprehensive support resources, community engagement through LinkedIn, and approaching potential setup challenges with patience as part of the learning process.\n",
    "\n",
    "### **Highlights**\n",
    "\n",
    "- **Broad Target Audience and Differentiated Learning Paths**: The course is structured to accommodate a wide range of learners, from individuals new to coding (who are provided with foundational guides) to seasoned AI agent engineers (who will find advanced material and challenging projects). Python coders and AI engineers are identified as being particularly well-positioned for the course content.\n",
    "- **Modern Development Environment with Cursor and UV**: Students will utilize **Cursor IDE**, an AI-powered editor built on VSCode, for enhanced productivity. A significant shift in environment management is the adoption of **UV**, an extremely fast, Rust-based Python package and virtual environment manager, presented as a simpler and more efficient alternative to tools like Anaconda.\n",
    "- **Open Discussion on API Costs and Free Alternatives**: The course addresses the financial aspect of using frontier LLM APIs (e.g., OpenAI, DeepSeek), noting the instructor's minimal expenses (under $5 for core course content). It provides clear alternatives for students wishing to avoid costs, such as using **Olama** to run open-source models locally or leveraging Gemini's free tier, while also explaining the associated trade-offs in performance and result quality.\n",
    "- **Rich Ecosystem of Learning Resources**: Students will benefit from a variety of support materials, including a dedicated course website with curated links, supplementary YouTube video content, and an extensive GitHub repository featuring skill-building guides, troubleshooting solutions, and regularly updated lab exercises described as a \"living, breathing resource.\"\n",
    "- **Valuing Patience and Problem-Solving in Setup**: The instructor acknowledges that configuring cutting-edge data science environments can present challenges. Students are encouraged to approach this phase with patience and view any encountered obstacles not as setbacks, but as integral and valuable learning opportunities.\n",
    "- **Emphasis on Instructor Support and Active Community Building**: Ed Donner highlights his commitment to being highly responsive to student questions and feedback via email and LinkedIn. He actively promotes the creation of a supportive learning community, encouraging students to connect, share their projects publicly (e.g., on LinkedIn, tagging him for amplification), and engage with fellow learners.\n",
    "- **UV's Growing Prominence**: The choice of UV is underscored by its speed, simplicity, and increasing adoption within the AI agent framework ecosystem itself (e.g., CrewAI is noted as being \"built with UV\"). This positions learners at the forefront of Python environment management trends.\n",
    "- **Contextualizing API Expenses**: While acknowledging potential frustration with \"nickel and diming\" API costs, the instructor provides context by explaining the immense computational power (trillions of calculations) involved in LLM inference, suggesting that API charges are relatively low given the underlying operational costs and the value received compared to purchasing high-end hardware.\n",
    "- **Structured Path to Environment Readiness**: The immediate next steps in the course involve dedicated video guides for setting up the development environment, with separate instructions tailored for PC users and Mac/Linux users, ensuring all students can establish a working setup before proceeding.\n",
    "\n",
    "### **Conceptual Understanding**\n",
    "\n",
    "- **UV for Python Environment and Package Management**\n",
    "    1. **Why is this concept important?** In Python development, especially in data science and AI where projects often have numerous and sometimes conflicting dependencies, managing isolated and reproducible environments is crucial. UV is introduced as a modern, high-performance tool written in Rust that aims to replace or augment traditional tools like `pip`, `venv`, and `conda`. Its primary benefits are significant speed improvements in resolving and installing packages, and a simplified user experience, which can drastically reduce time spent on environment setup and debugging dependency issues.\n",
    "    2. **How does it connect to real-world tasks, problems, or applications?** Data scientists and AI engineers frequently juggle multiple projects, each potentially requiring different versions of Python libraries. UV helps by:\n",
    "        - **Rapidly creating isolated virtual environments** for each project, preventing cross-project interference.\n",
    "        - **Dramatically speeding up the installation of dependencies** from `requirements.txt` or `pyproject.toml` files, which is particularly beneficial for projects with many packages.\n",
    "        - **Improving reproducibility** by making it easier and faster for team members or deployment systems to replicate the exact development environment.\n",
    "        Its adoption by agent frameworks themselves indicates its growing reliability and efficiency in the AI development ecosystem.\n",
    "    3. **Which related techniques or areas should be studied alongside this concept?**\n",
    "        - **Fundamentals of Python Virtual Environments:** Understanding why they are used (e.g., `venv` module in Python's standard library).\n",
    "        - **Python Packaging Standards:** Familiarity with `requirements.txt` for simple dependency lists and `pyproject.toml` for more complex project definitions (often used with tools like Poetry or PDM, though UV aims to provide a comprehensive solution).\n",
    "        - **Dependency Resolution Algorithms:** Gaining a high-level understanding of how package managers determine compatible sets of library versions.\n",
    "        - **Best Practices for Reproducible Research/Development:** Techniques beyond environment management that ensure code and results can be consistently reproduced.\n",
    "        - **Rust (Optional):** For those curious about UV's performance, knowing that Rust is a systems programming language focused on speed and memory safety provides context for its efficiency.\n",
    "\n",
    "### **Reflective Questions**\n",
    "\n",
    "1. **Application:** A data science team is evaluating new tools to streamline their project setup and reduce the time developers spend waiting for dependencies to install, which is a common bottleneck. Based on the course instructor's description, what specific advantages would UV offer this team over traditional tools like pip combined with venv, or Anaconda?\n",
    "    - *Answer:* UV would offer the team significantly faster environment creation and package installation times due to its Rust-based architecture, leading to less developer downtime. Its simplified approach to managing virtual environments and dependencies aims to reduce the complexity and potential for errors often encountered with traditional setups, making onboarding and project replication more efficient.\n",
    "2. **Teaching:** How would you explain the instructor's advice about API costs—being transparent about them while also offering free alternatives—to a prospective student who is very budget-conscious and worried about hidden expenses in AI courses?\n",
    "    - *Answer:* The instructor is upfront that using the absolute best AI models via APIs does have small costs, but he also shows you how he spent very little (under $5) for most of the course. More importantly, he provides completely free options, like using Olama to run models on your own computer, so you *don't have to spend anything* if you don't want to. This way, you can still follow along and learn, and just watch his demonstrations for the parts that use paid services, ensuring the course is accessible regardless of your budget.\n",
    "3. **Extension:** The instructor strongly encourages students to share their course projects on LinkedIn and tag him for amplification. Beyond personal visibility, how can this practice contribute to the broader learning and growth of the AI agent engineering community itself?\n",
    "    - *Answer:* When students share their projects and learnings on LinkedIn, it creates a public repository of diverse use cases, innovative solutions, and practical insights related to AI agent engineering. This can inspire other learners, spark discussions, help identify common challenges (and solutions), and showcase the capabilities being developed, effectively accelerating the collective knowledge and fostering a more vibrant and interconnected community of practice.\n",
    "\n",
    "# Day 1 - Windows Setup for AI Development: Git, Cursor IDE & UV Package Manager\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "This comprehensive guide walks PC users through the meticulous five-step process of setting up the development environment for Ed Donner's AI agent engineering course. It emphasizes using the course's GitHub repository as the primary source for instructions, including crucial \"Gotchas\" for Windows users, and details the installation and configuration of Git, the AI-powered Cursor IDE, and the fast, Rust-based UV environment and package manager. The final steps involve securing an OpenAI API key, understanding billing, and correctly storing this key (and potentially others for services like DeepSeek) in a local `.env` file to ensure secure and functional access to LLM services.\n",
    "\n",
    "### **Highlights**\n",
    "\n",
    "- **Primacy of Written Guides and \"Gotchas\"**: Users are strongly directed to consult the `README.md` and detailed Windows setup instructions in the course's GitHub repository. Special attention is called to the \"Gotchas\" section, which addresses common PC-specific issues like Windows file path length limitations (requiring a setting change if enabled) and potential interference from antivirus software.\n",
    "- **Step 1: Repository Cloning with Git**: The initial setup involves opening PowerShell, navigating to a user-defined `projects` directory (creating one if it doesn't exist), ensuring Git is installed (with guidance to download it if not), and then cloning the course repository using `git clone <repository_URL>`. The cloned directory (e.g., `agents`) becomes the project root.\n",
    "- **Step 2: Installing and Configuring Cursor IDE**: The Cursor IDE, an AI-assisted editor built on VSCode, is installed by downloading from its website and following the setup wizard. The cloned `agents` project folder is then opened within Cursor, which will serve as the primary development interface.\n",
    "- **Step 3: Adopting UV for Environment and Package Management**: UV, described as an \"extremely fast\" Rust-based Python package manager and environment tool (replacing Anaconda from previous courses), is installed via a PowerShell command. After installation and a shell restart (to update PATH), the command `uv sync` is run from within Cursor's integrated terminal in the project root. This command automatically installs a project-specific Python version and all dependencies, creating a local virtual environment (in a `.venv` folder) significantly faster than traditional methods.\n",
    "- **Executing Python Scripts via UV**: Once the environment is built with UV, Python scripts within the project are to be run using the `uv run <script_name.py>` command, ensuring they execute within the correct isolated environment.\n",
    "- **Step 4: OpenAI API Key Acquisition and Billing Setup**: Users are guided through setting up an account and billing (requiring a minimum $5 initial credit) on `platform.openai.com`. The process for creating a new secret API key is detailed, with alternatives like DeepSeek, Gemini, Grok, or OpenRouter mentioned for those facing issues or wishing to manage costs differently.\n",
    "- **Critical Advice for API Key Handling**: A crucial \"pro tip\" warns against pasting copied API keys into rich-text editors (like some notepads or word processors) that might automatically alter characters (e.g., hyphens or quotes), which can corrupt the key and cause authentication failures. Keys should be handled as plain text.\n",
    "- **Step 5: Securely Storing API Keys in a `.env` File**: The final setup stage involves creating a new file named exactly `.env` (a dot file) in the root of the project directory using Cursor. This file is used to store sensitive environment variables like API keys locally and must not be committed to version control.\n",
    "- **Correct `.env` File Formatting**: API keys are stored in the `.env` file using the format `VARIABLE_NAME=value`. For OpenAI, the variable must be `OPENAI_API_KEY=<your_actual_key>`. Similar entries can be made for other services (e.g., `DEEPSEEK_API_KEY`).\n",
    "- **Thorough and Sequential Process**: The setup is presented as a structured, multi-step process, with the instructor providing detailed explanations for each tool and action, aiming to simplify what can often be a complex and \"painful\" undertaking for cutting-edge data science environments.\n",
    "\n",
    "### **Conceptual Understanding**\n",
    "\n",
    "- **Using a `.env` File for API Keys and Secrets**\n",
    "    1. **Why is this concept important?** Embedding sensitive credentials like API keys directly into source code is a significant security vulnerability. If the code is shared, pushed to a public repository, or even a private one with broad access, these secrets are exposed. `.env` files offer a standard convention for developers to store such project-specific configuration variables locally. By adding the `.env` file to the project's `.gitignore` file, it is prevented from being committed to version control, thus keeping secrets out of the shared codebase. Applications are then typically configured to load these variables from the `.env` file into the runtime environment.\n",
    "    2. **How does it connect to real-world tasks, problems, or applications?** This practice is fundamental in virtually all modern software development that involves interaction with external authenticated services (e.g., cloud platforms, third-party APIs like OpenAI, databases):\n",
    "        - It allows each team member to use their own API keys or credentials in their local development environment without affecting others.\n",
    "        - It facilitates different configurations for different environments (development, staging, production) by using distinct `.env` files or equivalent environment variable settings on servers.\n",
    "        - It is a cornerstone of secure credential management, preventing accidental leakage of sensitive information.\n",
    "    3. **Which related techniques or areas should be studied alongside this concept?**\n",
    "        - **Environment Variables:** Understanding how operating systems and applications utilize environment variables for configuration.\n",
    "        - **`.gitignore` Files:** Learning the syntax and purpose of `.gitignore` to exclude files and directories from Git tracking.\n",
    "        - **Secrets Management Platforms:** For production environments and larger teams, more robust solutions like HashiCorp Vault, AWS Secrets Manager, Azure Key Vault, or Google Cloud Secret Manager are used for managing secrets securely.\n",
    "        - **Application Configuration Patterns:** General best practices for managing application settings across various deployment stages.\n",
    "        - **Security Best Practices:** Broader principles for secure software development, including input validation, output encoding, and secure handling of sensitive data.\n",
    "\n",
    "### **Code Examples (Command-Line and `.env` File Structure)**\n",
    "\n",
    "1. **Check Git Installation (PowerShell):**\n",
    "    \n",
    "    `git --version`\n",
    "    \n",
    "2. Clone GitHub Repository (PowerShell):\n",
    "    \n",
    "    (Ensure you are in your desired parent directory, e.g., cd ~/projects)\n",
    "    \n",
    "    `git clone <URL_of_the_course_repository> agents`\n",
    "    \n",
    "    *(Replace `<URL_of_the_course_repository>` with the actual URL; `agents` will be the name of the cloned folder).*\n",
    "    \n",
    "3. **Navigate into Cloned Project Directory (PowerShell):**\n",
    "    \n",
    "    `cd agents`\n",
    "    \n",
    "4. **Check UV Installation (PowerShell - after installing UV and restarting PowerShell):**\n",
    "    \n",
    "    `uv --version`\n",
    "    \n",
    "5. **Create Environment and Install Dependencies with UV (PowerShell - from the project root directory, e.g., `agents`):**\n",
    "    \n",
    "    `uv sync`\n",
    "    \n",
    "6. **Run a Python Script using UV-managed Environment (PowerShell - from the project root):**\n",
    "    \n",
    "    `uv run your_script.py`\n",
    "    \n",
    "    *(Replace `your_script.py` with the actual script name).*\n",
    "    \n",
    "7. **Content and Structure of the `.env` File (Create this file in the project root, e.g., `agents/.env`):**\n",
    "    \n",
    "    **Ini, TOML**\n",
    "    \n",
    "    `# This is a comment, actual lines should not start with # unless intended as such\n",
    "    OPENAI_API_KEY=sk-YourActualOpenAIApiKeyStartingWith_sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "    DEEPSEEK_API_KEY=YourActualDeepSeekApiKeyIfApplicablexxxxxxxxxx\n",
    "    # Add any other required API keys or environment-specific variables here\n",
    "    # Example: GOOGLE_API_KEY=YourGoogleApiKeyIfApplicablexxxxxxxxxxx`\n",
    "    \n",
    "    **Important**: Replace placeholder values with your actual secret keys. Ensure there are no extra spaces around the `=` sign. This file should be listed in your `.gitignore` file to prevent committing it.\n",
    "    \n",
    "\n",
    "### **Reflective Questions**\n",
    "\n",
    "1. **Application:** A student new to managing Python projects is confused about why `uv sync` is preferred in this course over just using `pip install` directly. How would the creation of a `.venv` folder by `uv sync` and the use of `uv run` benefit them in keeping their project dependencies organized and avoiding conflicts with other Python projects on their system?\n",
    "    - *Answer:* `uv sync` creates an isolated \"sandbox\" (the `.venv` folder) specifically for this course's project, installing the exact Python version and library versions needed, separate from any other Python installations or projects on their computer. Using `uv run` then ensures scripts use *only* this sandboxed environment, which prevents libraries from one project interfering with another (e.g., needing different versions of the same library), a common source of errors and frustration when dependencies are managed globally or haphazardly.\n",
    "2. **Teaching:** How would you explain the instructor's \"pro tip\" about not pasting API keys into rich text editors to a non-technical friend to help them understand the potential risk in a simple way?\n",
    "    - *Answer:* Think of an API key like a very precise magic password for a computer system. If you paste this \"magic password\" into a fancy text editor like Microsoft Word or even some basic note apps, the app might try to be \"helpful\" by automatically changing things like hyphens to longer dashes or straight quotes to curly quotes to make it look prettier. But the computer system expecting the magic password is very strict; if even one tiny character is different from the original, the password won't work, and it can be incredibly hard to figure out why. It's best to treat these keys like raw code and only paste them into plain text tools or directly where they are needed.\n",
    "3. **Extension:** The setup process involves multiple tools (Git, Cursor, UV, PowerShell) and external services (GitHub, OpenAI). What general troubleshooting strategy should a student adopt if they encounter an error at any step, beyond just re-watching the video or re-reading the specific instruction?\n",
    "    - *Answer:* A good general troubleshooting strategy would be to:\n",
    "        1. **Precisely copy the error message:** Use this to search online (e.g., Google, Stack Overflow) as many common setup issues have known solutions.\n",
    "        2. **Check the \"Gotchas\" and troubleshooting guides:** The instructor mentioned these are in the GitHub repo and will be updated.\n",
    "        3. **Verify prerequisites:** Ensure previous steps were completed successfully (e.g., is Git actually in PATH? Did PowerShell restart after an install that changed PATH?).\n",
    "        4. **Isolate the problem:** If `uv sync` fails, is it a network issue, a problem with a specific package, or UV itself? Check UV's documentation or issue tracker.\n",
    "        5. **Consult course Q&amp;A/community:** See if other students have faced and resolved similar issues.\n",
    "        6. **Contact the instructor:** As a last resort, providing clear details of the problem and steps taken.\n",
    "\n",
    "# Day 1 - Building Your First Agentic AI Workflow with OpenAI API: Step-by-Step\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "This session marks the first practical lab after environment setup, guiding students (Mac, PC, and Linux users reunited) through interacting with the OpenAI API using Python in a Cursor IDE Jupyter notebook. Key steps include selecting the correct Python kernel from the UV-created `.venv`, securely loading the `OPENAI_API_KEY` from the `.env` file using `python-dotenv` with the `override=True` flag, and instantiating the OpenAI client. Students then make their first API calls to \"gpt-4-mini\" (or a similar model), demonstrate a simple multi-step \"agentic\" pattern (LLM proposes a question, then another LLM call answers it), and conclude with an exercise to apply this pattern to identify a business opportunity, a related pain point, and an AI solution.\n",
    "\n",
    "### **Highlights**\n",
    "\n",
    "- **Post-Setup Lab Environment**: Students begin by opening `Lab_1.ipynb` in the `one_foundations` (Week 1) directory within Cursor and are critically reminded to select the Python kernel associated with their project-specific virtual environment (e.g., `.venv` created by UV, displaying a version like Python 3.12).\n",
    "- **Utilizing Course Resources**: The instructor emphasizes the `guides` folder within the course repository, which contains valuable self-study materials for Python (beginner to intermediate, including `async` and `generators`), debugging common errors like `NameError`, and a comprehensive troubleshooting guide.\n",
    "- **\"Living E-book\" Lab Notebooks**: Lab notebooks are presented as dynamic resources that receive continuous updates (new information, clarifications, fixes) based on student feedback and evolving content. Students should always refer to the latest version in the GitHub repository, understanding it might differ from older video recordings.\n",
    "- **Secure API Key Management with `python-dotenv`**: The lab demonstrates the best practice of loading sensitive API keys using the `python-dotenv` library. The code `from dotenv import load_dotenv; load_dotenv(override=True)` is used to read variables from the `.env` file in the project root, with `override=True` ensuring these project-specific keys take precedence over any globally set environment variables.\n",
    "- **OpenAI API Interaction**: Students learn the standard procedure for using the OpenAI Python library:\n",
    "    1. Import the `OpenAI` class: `from openai import OpenAI`.\n",
    "    2. Instantiate the client: `client = OpenAI()` (which implicitly uses the `OPENAI_API_KEY` loaded into the environment).\n",
    "    3. Make requests to the chat completions endpoint: `response = client.chat.completions.create(model=\"gpt-4-mini\", messages=messages)`.\n",
    "    4. Process the response: `reply_content = response.choices[0].message.content`.\n",
    "- **AI-Assisted Coding with Cursor**: The session showcases Cursor's capability to provide intelligent code suggestions and auto-completions for common tasks like importing libraries and structuring API calls, aiding productivity.\n",
    "- **Introduction to a Simple \"Agentic\" Pattern**: A basic multi-step workflow is introduced where an LLM is first prompted to generate a challenging IQ question, and then the generated question is used as input for a second LLM call to produce an answer. This illustrates a foundational concept of orchestrating multiple LLM interactions to achieve a more complex outcome.\n",
    "- **Displaying Formatted Output with Markdown**: The lab shows how to use `IPython.display.display` and `IPython.display.Markdown` to render LLM responses containing Markdown in a properly formatted way within the Jupyter notebook, enhancing readability.\n",
    "- **Practical Application Exercise**: The lab concludes with an exercise requiring students to apply the multi-LLM call pattern to a business scenario: 1) prompting an LLM to identify a business area ripe for AI, 2) then prompting for a specific pain point in that area, and 3) finally prompting for a proposed AI solution to address that pain point.\n",
    "- **Role of Jupyter Notebooks in Learning**: The instructor reiterates the value of Jupyter notebooks for educational purposes and research and development, as they facilitate step-by-step execution, experimentation, and easy inspection of variables and outputs, despite some engineers' preference for traditional Python modules for production code.\n",
    "\n",
    "### **Conceptual Understanding**\n",
    "\n",
    "- **Simple Multi-Step LLM Orchestration (Semi-Agentic Pattern)**\n",
    "    1. **Why is this concept important?** This pattern is a foundational building block for more complex AI agent behavior. It demonstrates that a larger task can be broken down into smaller, manageable steps, with each step potentially involving an LLM interaction. The output of one step can dynamically inform the input of the next, allowing for a chain of reasoning or action. This moves beyond single-turn Q&amp;A and towards systems that can perform sequences of operations to achieve a goal.\n",
    "    2. **How does it connect to real-world tasks, problems, or applications?** This simple orchestration is a precursor to how sophisticated AI agents operate:\n",
    "        - **Information Gathering & Summarization:** An LLM might first identify relevant documents based on a query, and a subsequent LLM call could summarize the key findings from those documents.\n",
    "        - **Iterative Content Creation:** An LLM could generate an initial draft, then a second call (perhaps with different instructions or incorporating feedback) could refine or expand upon it.\n",
    "        - **Problem Decomposition & Solution:** As seen in the lab (generating a question then answering it) or the exercise (identifying a business problem then proposing a solution).\n",
    "        Even basic chatbots often use a similar pattern internally, for instance, to understand user intent and then retrieve information before formulating a final response.\n",
    "    3. **Which related techniques or areas should be studied alongside this concept?**\n",
    "        - **AI Agent Architectures:** More formal frameworks for building agents, such as ReAct (Reason + Act), which explicitly model thought processes and actions.\n",
    "        - **Planning Algorithms in AI:** Techniques that allow an agent to devise a sequence of steps to reach a desired state or goal.\n",
    "        - **State Management in Conversational AI:** Methods for keeping track of information and context across multiple turns or steps in an interaction.\n",
    "        - **LLM Function Calling / Tool Use:** A more advanced capability where LLMs can decide to call specific predefined functions (tools) and use their outputs to continue processing, enabling interaction with external systems.\n",
    "        - **Workflow Automation Platforms:** Tools like LangChain, CrewAI, or even visual workflow builders (as seen in a prior lecture's n8n demo) that provide higher-level abstractions for creating and managing these multi-step LLM chains or agentic processes.\n",
    "\n",
    "### **Code Examples (Python snippets from the lab)**\n",
    "\n",
    "1. **Loading API Keys from `.env` File:**\n",
    "    \n",
    "    ```python\n",
    "    from dotenv import load_dotenv\n",
    "    \n",
    "    # Ensures .env variables in the project root take precedence\n",
    "    load_dotenv(override=True)\n",
    "    \n",
    "    ```\n",
    "    \n",
    "2. **Checking for and Displaying a Loaded Environment Variable:**\n",
    "    \n",
    "    ```python\n",
    "    import os\n",
    "    \n",
    "    openai_api_key = os.getenv(\"OPENAI_API_KEY\") # Case-sensitive\n",
    "    if openai_api_key:\n",
    "        print(f\"OpenAI API Key successfully loaded and starts with: {openai_api_key[:7]}...\")\n",
    "    else:\n",
    "        print(\"Error: OPENAI_API_KEY not found. Check your .env file and setup.\")\n",
    "    \n",
    "    ```\n",
    "    \n",
    "3. **Standard OpenAI API Call Structure:**\n",
    "    \n",
    "    ```python\n",
    "    from openai import OpenAI\n",
    "    \n",
    "    # Client automatically picks up OPENAI_API_KEY from environment\n",
    "    client = OpenAI()\n",
    "    \n",
    "    # Example messages list\n",
    "    messages_payload = [\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
    "    ]\n",
    "    \n",
    "    # Making the API call\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4-mini\", # Model used in the lab example\n",
    "        messages=messages_payload\n",
    "    )\n",
    "    \n",
    "    # Extracting the text content from the response\n",
    "    reply = response.choices[0].message.content\n",
    "    print(reply)\n",
    "    \n",
    "    ```\n",
    "    \n",
    "4. **Rendering Markdown Output in Jupyter Notebooks:**\n",
    "    \n",
    "    ```python\n",
    "    from IPython.display import display, Markdown\n",
    "    \n",
    "    # Assume 'llm_markdown_output' is a string variable containing Markdown from an LLM\n",
    "    llm_markdown_output = \"## Solution Steps\\n\\n1. **First step**\\n2. *Second step*\"\n",
    "    display(Markdown(llm_markdown_output))\n",
    "    \n",
    "    ```\n",
    "    \n",
    "5. **Conceptual Flow for the Two-Step \"Agentic\" IQ Question Pattern:**\n",
    "    \n",
    "    ```python\n",
    "    # client = OpenAI() # Assumed to be instantiated\n",
    "    \n",
    "    # --- Step 1: Generate a challenging IQ question ---\n",
    "    prompt_for_question = \"Please propose a hard, challenging question to assess someone's IQ. Respond only with the question.\"\n",
    "    messages_for_generating_question = [{\"role\": \"user\", \"content\": prompt_for_question}]\n",
    "    \n",
    "    response_with_question = client.chat.completions.create(\n",
    "        model=\"gpt-4-mini\",\n",
    "        messages=messages_for_generating_question\n",
    "    )\n",
    "    generated_iq_question = response_with_question.choices[0].message.content\n",
    "    print(f\"Generated IQ Question: {generated_iq_question}\")\n",
    "    \n",
    "    # --- Step 2: Answer the generated IQ question ---\n",
    "    messages_for_answering_question = [{\"role\": \"user\", \"content\": generated_iq_question}]\n",
    "    \n",
    "    response_with_answer = client.chat.completions.create(\n",
    "        model=\"gpt-4-mini\",\n",
    "        messages=messages_for_answering_question\n",
    "    )\n",
    "    answer_to_iq_question = response_with_answer.choices[0].message.content\n",
    "    \n",
    "    # --- Step 3: Display the answer (potentially with Markdown formatting) ---\n",
    "    # from IPython.display import display, Markdown # if needed\n",
    "    # display(Markdown(answer_to_iq_question))\n",
    "    print(f\"\\nAnswer to IQ Question:\\n{answer_to_iq_question}\")\n",
    "    \n",
    "    ```\n",
    "    \n",
    "\n",
    "### **Reflective Questions**\n",
    "\n",
    "1. **Application:** The lab exercise involves a three-step LLM chain (Business Area -> Pain Point -> AI Solution). If you were to extend this to create a mini \"startup idea generator,\" what fourth LLM call could you add to make the output even more practical for someone looking to start a business?\n",
    "    - *Answer:* A fourth LLM call could take the \"AI Solution\" and prompt the model to: \"Outline 3-5 key features for a Minimum Viable Product (MVP) for this AI solution and suggest a catchy name for this startup idea.\" This would provide more concrete, actionable steps beyond just the conceptual solution.\n",
    "2. **Teaching:** How would you explain the importance of `override=True` in `load_dotenv(override=True)` to a fellow student who says, \"I already set my OpenAI key in my system's environment variables, so I don't think I need this `override` part\"?\n",
    "    - *Answer:* While your globally set key might work now, using `override=True` with a project-specific `.env` file is a best practice for consistency and avoiding future headaches. It ensures that *this specific project* always uses the key you define in *its own `.env` file*, regardless of what's set globally. This is super helpful if you later work on another project that needs a different key, or if your global key expires or changes – this project will keep working correctly with its dedicated key, making it more reliable and easier to manage.\n",
    "3. **Extension:** The lab uses \"gpt-4-mini.\" If a student decided to replace \"gpt-4-mini\" with a much smaller, locally run open-source model (via Olama, assuming a compatible API setup) for the multi-step IQ question pattern, what potential differences in the quality and nature of the interaction (question generation and answering) should they anticipate?\n",
    "    - *Answer:* They should anticipate that a much smaller, local model might generate simpler or less nuanced IQ questions, potentially even common riddles rather than truly challenging problems. For the answering step, the smaller model might struggle with complex reasoning, provide less detailed explanations, or even make errors in solving the problem it (or another model) generated. The overall coherence and depth of the interaction would likely be reduced compared to using a frontier model like \"gpt-4-mini.\"\n",
    "\n",
    "# Day 1 - Introduction to Agentic AI: Creating Multi-Step LLM Workflows + Autonomy\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "This instructional segment marks the completion of the first day of an AI agent engineering course, emphasizing that students have not only navigated the setup but also executed their first multi-step \"agentic workflow.\" A key learning highlighted is the LLM's \"baby step\" towards autonomy in the lab exercise by independently selecting a business sector for analysis, a hallmark of agentic AI. The instructor acknowledges the length of Day 1 and assures that future sessions will progress more briskly, with the next focus being a deeper dive into the definitions and mechanics of AI agents and agentic patterns.\n",
    "\n",
    "### **Highlights**\n",
    "\n",
    "- **Completion of First Agentic Workflow**: Students have successfully implemented a multi-step workflow using Large Language Models, gaining practical experience and, through the associated exercise, insights into potential commercial applications.\n",
    "- **Initial Glimpse of LLM Autonomy**: A significant outcome of the first lab was observing a nascent form of LLM autonomy, where the model was tasked to choose its own area of investigation (a business sector). This ability to \"plot its own path\" or \"choose its own adventure\" is noted as a fundamental characteristic of agentic AI systems.\n",
    "- **Day 1 Progress and Future Pace**: While the first day was extensive due to environment setup, it laid crucial groundwork. Subsequent course days are expected to be shorter and allow for more rapid advancement into building commercially relevant AI applications.\n",
    "- **Forthcoming Deeper Dive into Agent Theory**: The next phase of the course will formally introduce and explore the core concepts of AI agents and agentic patterns, providing detailed explanations of what they are and how they function, building on the practical introduction.\n",
    "\n",
    "### **Conceptual Understanding**\n",
    "\n",
    "- **Nascent LLM Autonomy in Orchestrated Workflows**\n",
    "    1. **Why is this concept important?** When an LLM makes a choice within a workflow (e.g., selecting a business sector to analyze based on a general prompt), it demonstrates a basic level of autonomy. This signifies a departure from simply executing explicit, detailed instructions for every step. This capacity for self-direction, even if minor, is a foundational element of how more sophisticated AI agents operate, enabling them to navigate complex tasks with less prescriptive guidance.\n",
    "    2. **How does it connect to real-world tasks, problems, or applications?** This \"baby step\" towards autonomy is a simplified version of how advanced AI agents function. In more complex scenarios, an agent might autonomously decide which database to query, what sequence of tools to use, or how to best approach a problem based on its understanding of the goal and its available resources. For instance, a research agent might autonomously choose which keywords to use for a literature search or which papers to prioritize for summarization.\n",
    "    3. **Which related techniques or areas should be studied alongside this concept?** To understand how this autonomy is developed and managed, related areas include:\n",
    "        - **LLM Reasoning and Decision-Making:** How LLMs can be prompted or fine-tuned to make logical choices or inferences.\n",
    "        - **Planning Algorithms in AI:** Formal methods for enabling agents to construct sequences of actions to achieve goals.\n",
    "        - **Tool Use and Function Calling:** Mechanisms that allow LLMs to select and utilize external tools or APIs based on the task at hand.\n",
    "        - **Reinforcement Learning:** Particularly how it can be used to train agents to make optimal decisions in dynamic environments.\n",
    "        - **Agent Architectures (e.g., ReAct):** Frameworks that explicitly model cycles of thought, action, and observation to guide agent behavior.\n",
    "\n",
    "### **Reflective Questions**\n",
    "\n",
    "1. **Application:** The lab exercise demonstrated the LLM autonomously choosing a business sector. If you were designing an AI agent to help with academic research, what similar autonomous decision could the agent make early in its workflow to narrow down its research focus effectively?\n",
    "    - *Answer:* An academic research agent could be prompted with a broad research interest and then autonomously identify 2-3 recent, highly-cited sub-themes or emerging questions within that area by performing a preliminary scan of review articles or conference proceedings, before committing to a deeper dive into one chosen sub-theme.\n",
    "2. **Teaching:** How would you explain the significance of the LLM \"choosing its own adventure\" by selecting a business sector, compared to a user explicitly telling it which sector to analyze, to someone who is just starting to learn about AI agents?\n",
    "    - *Answer:* When a user specifies the sector, the LLM is just following a direct order. When the LLM *chooses* the sector itself based on a more general instruction (like \"find an AI opportunity\"), it's making a decision based on its training and the prompt's context. This act of selection, however small, is a step towards self-directed behavior, which is a core idea in making AI agents more independent and capable of tackling complex problems without every single step being dictated."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
